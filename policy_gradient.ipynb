{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tkinter as tk\n",
    "from PIL import ImageTk, Image\n",
    "\n",
    "import copy\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PhotoImage = ImageTk.PhotoImage\n",
    "UNIT = 50  # 픽셀 수\n",
    "HEIGHT = 5  # 그리드 세로\n",
    "WIDTH = 5  # 그리드 가로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env(tk.Tk):\n",
    "    def __init__(self, render_speed=0.01):\n",
    "        super(Env, self).__init__()\n",
    "        self.render_speed=render_speed\n",
    "        self.action_space = ['u', 'd', 'l', 'r']\n",
    "        self.action_size = len(self.action_space)\n",
    "        self.title('REINFORCE')\n",
    "        self.geometry('{0}x{1}'.format(HEIGHT * UNIT, HEIGHT * UNIT))\n",
    "        self.shapes = self.load_images()\n",
    "        self.canvas = self._build_canvas()\n",
    "        self.counter = 0\n",
    "        self.rewards = []\n",
    "        self.goal = []\n",
    "        # 장애물 설정\n",
    "        self.set_reward([0, 1], -1)\n",
    "        self.set_reward([1, 2], -1)\n",
    "        self.set_reward([2, 3], -1)\n",
    "        # 목표 지점 설정\n",
    "        self.set_reward([4, 4], 1)\n",
    "\n",
    "    def _build_canvas(self):\n",
    "        canvas = tk.Canvas(self, bg='white',\n",
    "                           height=HEIGHT * UNIT,\n",
    "                           width=WIDTH * UNIT)\n",
    "        # 그리드 생성\n",
    "        for c in range(0, WIDTH * UNIT, UNIT):  # 0~400 by 80\n",
    "            x0, y0, x1, y1 = c, 0, c, HEIGHT * UNIT\n",
    "            canvas.create_line(x0, y0, x1, y1)\n",
    "        for r in range(0, HEIGHT * UNIT, UNIT):  # 0~400 by 80\n",
    "            x0, y0, x1, y1 = 0, r, HEIGHT * UNIT, r\n",
    "            canvas.create_line(x0, y0, x1, y1)\n",
    "\n",
    "        self.rewards = []\n",
    "        self.goal = []\n",
    "        # 캔버스에 이미지 추가\n",
    "        x, y = UNIT/2, UNIT/2\n",
    "        self.rectangle = canvas.create_image(x, y, image=self.shapes[0])\n",
    "\n",
    "        canvas.pack()\n",
    "\n",
    "        return canvas\n",
    "\n",
    "    def load_images(self):\n",
    "        rectangle = PhotoImage(\n",
    "            Image.open(\"./img/rectangle.png\").resize((30, 30)))\n",
    "        triangle = PhotoImage(\n",
    "            Image.open(\"./img/triangle.png\").resize((30, 30)))\n",
    "        circle = PhotoImage(\n",
    "            Image.open(\"./img/circle.png\").resize((30, 30)))\n",
    "\n",
    "        return rectangle, triangle, circle\n",
    "\n",
    "    def reset_reward(self):\n",
    "\n",
    "        for reward in self.rewards:\n",
    "            self.canvas.delete(reward['figure'])\n",
    "\n",
    "        self.rewards.clear()\n",
    "        self.goal.clear()\n",
    "        self.set_reward([0, 1], -1)\n",
    "        self.set_reward([1, 2], -1)\n",
    "        self.set_reward([2, 3], -1)\n",
    "\n",
    "        # #goal\n",
    "        self.set_reward([4, 4], 1)\n",
    "\n",
    "    def set_reward(self, state, reward):\n",
    "        state = [int(state[0]), int(state[1])]\n",
    "        x = int(state[0])\n",
    "        y = int(state[1])\n",
    "        temp = {}\n",
    "        if reward > 0:\n",
    "            temp['reward'] = reward\n",
    "            temp['figure'] = self.canvas.create_image((UNIT * x) + UNIT / 2,\n",
    "                                                       (UNIT * y) + UNIT / 2,\n",
    "                                                       image=self.shapes[2])\n",
    "\n",
    "            self.goal.append(temp['figure'])\n",
    "\n",
    "\n",
    "        elif reward < 0:\n",
    "            temp['direction'] = -1\n",
    "            temp['reward'] = reward\n",
    "            temp['figure'] = self.canvas.create_image((UNIT * x) + UNIT / 2,\n",
    "                                                      (UNIT * y) + UNIT / 2,\n",
    "                                                      image=self.shapes[1])\n",
    "\n",
    "        temp['coords'] = self.canvas.coords(temp['figure'])\n",
    "        temp['state'] = state\n",
    "        self.rewards.append(temp)\n",
    "\n",
    "    # new methods\n",
    "    def check_if_reward(self, state):\n",
    "        check_list = dict()\n",
    "        check_list['if_goal'] = False\n",
    "        rewards = 0\n",
    "\n",
    "        for reward in self.rewards:\n",
    "            if reward['state'] == state:\n",
    "                rewards += reward['reward']\n",
    "                if reward['reward'] == 1:\n",
    "                    check_list['if_goal'] = True\n",
    "\n",
    "        check_list['rewards'] = rewards\n",
    "\n",
    "        return check_list\n",
    "\n",
    "    def coords_to_state(self, coords):\n",
    "        x = int((coords[0] - UNIT / 2) / UNIT)\n",
    "        y = int((coords[1] - UNIT / 2) / UNIT)\n",
    "        return [x, y]\n",
    "\n",
    "    def reset(self):\n",
    "        self.update()\n",
    "        time.sleep(0.5)\n",
    "        x, y = self.canvas.coords(self.rectangle)\n",
    "        self.canvas.move(self.rectangle, UNIT / 2 - x, UNIT / 2 - y)\n",
    "        self.reset_reward()\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        self.counter += 1\n",
    "        self.render()\n",
    "\n",
    "        if self.counter % 2 == 1:\n",
    "            self.rewards = self.move_rewards()\n",
    "\n",
    "        next_coords = self.move(self.rectangle, action)\n",
    "        check = self.check_if_reward(self.coords_to_state(next_coords))\n",
    "        done = check['if_goal']\n",
    "        reward = check['rewards']\n",
    "\n",
    "        self.canvas.tag_raise(self.rectangle)\n",
    "\n",
    "        s_ = self.get_state()\n",
    "\n",
    "        return s_, reward, done\n",
    "\n",
    "    def get_state(self):\n",
    "\n",
    "        location = self.coords_to_state(self.canvas.coords(self.rectangle))\n",
    "        agent_x = location[0]\n",
    "        agent_y = location[1]\n",
    "\n",
    "        states = list()\n",
    "\n",
    "        for reward in self.rewards:\n",
    "            reward_location = reward['state']\n",
    "            states.append(reward_location[0] - agent_x)\n",
    "            states.append(reward_location[1] - agent_y)\n",
    "            if reward['reward'] < 0:\n",
    "                states.append(-1)\n",
    "                states.append(reward['direction'])\n",
    "            else:\n",
    "                states.append(1)\n",
    "\n",
    "        return states\n",
    "\n",
    "    def move_rewards(self):\n",
    "        new_rewards = []\n",
    "        for temp in self.rewards:\n",
    "            if temp['reward'] == 1:\n",
    "                new_rewards.append(temp)\n",
    "                continue\n",
    "            temp['coords'] = self.move_const(temp)\n",
    "            temp['state'] = self.coords_to_state(temp['coords'])\n",
    "            new_rewards.append(temp)\n",
    "        return new_rewards\n",
    "\n",
    "    def move_const(self, target):\n",
    "\n",
    "        s = self.canvas.coords(target['figure'])\n",
    "\n",
    "        base_action = np.array([0, 0])\n",
    "\n",
    "        if s[0] == (WIDTH - 1) * UNIT + UNIT / 2:\n",
    "            target['direction'] = 1\n",
    "        elif s[0] == UNIT / 2:\n",
    "            target['direction'] = -1\n",
    "\n",
    "        if target['direction'] == -1:\n",
    "            base_action[0] += UNIT\n",
    "        elif target['direction'] == 1:\n",
    "            base_action[0] -= UNIT\n",
    "\n",
    "        if (target['figure'] is not self.rectangle\n",
    "           and s == [(WIDTH - 1) * UNIT, (HEIGHT - 1) * UNIT]):\n",
    "            base_action = np.array([0, 0])\n",
    "\n",
    "        self.canvas.move(target['figure'], base_action[0], base_action[1])\n",
    "\n",
    "        s_ = self.canvas.coords(target['figure'])\n",
    "\n",
    "        return s_\n",
    "\n",
    "    def move(self, target, action):\n",
    "        s = self.canvas.coords(target)\n",
    "\n",
    "        base_action = np.array([0, 0])\n",
    "\n",
    "        if action == 0:  # 상\n",
    "            if s[1] > UNIT:\n",
    "                base_action[1] -= UNIT\n",
    "        elif action == 1:  # 하\n",
    "            if s[1] < (HEIGHT - 1) * UNIT:\n",
    "                base_action[1] += UNIT\n",
    "        elif action == 2:  # 우\n",
    "            if s[0] < (WIDTH - 1) * UNIT:\n",
    "                base_action[0] += UNIT\n",
    "        elif action == 3:  # 좌\n",
    "            if s[0] > UNIT:\n",
    "                base_action[0] -= UNIT\n",
    "\n",
    "        self.canvas.move(target, base_action[0], base_action[1])\n",
    "\n",
    "        s_ = self.canvas.coords(target)\n",
    "\n",
    "        return s_\n",
    "\n",
    "    def render(self):\n",
    "        # 게임 속도 조정\n",
    "        time.sleep(self.render_speed)\n",
    "        self.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태가 입력, 각 행동의 확률이 출력인 인공신경망 생성\n",
    "class REINFORCE(tf.keras.Model):\n",
    "    def __init__(self, action_size):\n",
    "        super(REINFORCE, self).__init__()\n",
    "        self.fc1 = Dense(24, activation='relu')\n",
    "        self.fc2 = Dense(24, activation='relu')\n",
    "        self.fc_out = Dense(action_size, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        policy = self.fc_out(x)\n",
    "        return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그리드월드 예제에서의 REINFORCE 에이전트\n",
    "class REINFORCEAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # 상태의 크기와 행동의 크기 정의\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # REINFORCE 하이퍼 파라메터\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "\n",
    "        self.model = REINFORCE(self.action_size)\n",
    "        self.optimizer = Adam(lr=self.learning_rate)\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "\n",
    "    # 정책신경망으로 행동 선택\n",
    "    def get_action(self, state):\n",
    "        policy = self.model(state)[0]\n",
    "        policy = np.array(policy)\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0]\n",
    "\n",
    "    # 반환값 계산\n",
    "    def discount_rewards(self, rewards):\n",
    "        discounted_rewards = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            running_add = running_add * self.discount_factor + rewards[t]\n",
    "            discounted_rewards[t] = running_add\n",
    "        return discounted_rewards\n",
    "\n",
    "    # 한 에피소드 동안의 상태, 행동, 보상을 저장\n",
    "    def append_sample(self, state, action, reward):\n",
    "        self.states.append(state[0])\n",
    "        self.rewards.append(reward)\n",
    "        act = np.zeros(self.action_size)\n",
    "        act[action] = 1\n",
    "        self.actions.append(act)\n",
    "\n",
    "    # 정책신경망 업데이트\n",
    "    def train_model(self):\n",
    "        discounted_rewards = np.float32(self.discount_rewards(self.rewards))\n",
    "        discounted_rewards -= np.mean(discounted_rewards)\n",
    "        discounted_rewards /= np.std(discounted_rewards)\n",
    "        \n",
    "        # 크로스 엔트로피 오류함수 계산\n",
    "        model_params = self.model.trainable_variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(model_params)\n",
    "            policies = self.model(np.array(self.states))\n",
    "            actions = np.array(self.actions)\n",
    "            action_prob = tf.reduce_sum(actions * policies, axis=1)\n",
    "            cross_entropy = - tf.math.log(action_prob + 1e-5)\n",
    "            loss = tf.reduce_sum(cross_entropy * discounted_rewards)\n",
    "            entropy = - policies * tf.math.log(policies)\n",
    "\n",
    "        # 오류함수를 줄이는 방향으로 모델 업데이트\n",
    "        grads = tape.gradient(loss, model_params)\n",
    "        self.optimizer.apply_gradients(zip(grads, model_params))\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "        return np.mean(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경과 에이전트 생성\n",
    "env = Env(render_speed=0.01)\n",
    "state_size = 15\n",
    "action_space = [0, 1, 2, 3, 4]\n",
    "action_size = len(action_space)\n",
    "agent = REINFORCEAgent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, episodes = [], []\n",
    "\n",
    "EPISODES = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:   0 | score: -13 | entropy: 0.279\n",
      "episode:   1 | score: -12 | entropy: 0.279\n",
      "episode:   2 | score:  -3 | entropy: 0.269\n",
      "episode:   3 | score: -22 | entropy: 0.278\n",
      "episode:   4 | score:  -3 | entropy: 0.280\n",
      "episode:   5 | score:  -4 | entropy: 0.276\n",
      "episode:   6 | score:  -2 | entropy: 0.277\n",
      "episode:   7 | score:  -5 | entropy: 0.277\n",
      "episode:   8 | score:   0 | entropy: 0.280\n",
      "episode:   9 | score:   0 | entropy: 0.276\n",
      "episode:  10 | score:  -7 | entropy: 0.275\n",
      "episode:  11 | score: -21 | entropy: 0.274\n",
      "episode:  12 | score:  -1 | entropy: 0.274\n",
      "episode:  13 | score:  -2 | entropy: 0.273\n",
      "episode:  14 | score: -12 | entropy: 0.274\n",
      "episode:  15 | score:  -5 | entropy: 0.275\n",
      "episode:  16 | score:  -8 | entropy: 0.276\n",
      "episode:  17 | score:  -4 | entropy: 0.273\n",
      "episode:  18 | score: -10 | entropy: 0.273\n",
      "episode:  19 | score:  -2 | entropy: 0.276\n",
      "episode:  20 | score:  -1 | entropy: 0.271\n",
      "episode:  21 | score:   1 | entropy: 0.279\n",
      "episode:  22 | score:   0 | entropy: 0.270\n",
      "episode:  23 | score:  -7 | entropy: 0.271\n",
      "episode:  24 | score:   0 | entropy: 0.271\n",
      "episode:  25 | score:   1 | entropy: 0.269\n",
      "episode:  26 | score:  -7 | entropy: 0.270\n",
      "episode:  27 | score:   0 | entropy: 0.273\n",
      "episode:  28 | score:  -2 | entropy: 0.271\n",
      "episode:  29 | score:  -5 | entropy: 0.275\n",
      "episode:  30 | score:   0 | entropy: 0.270\n",
      "episode:  31 | score:   0 | entropy: 0.269\n",
      "episode:  32 | score:   1 | entropy: 0.267\n",
      "episode:  33 | score:  -4 | entropy: 0.274\n",
      "episode:  34 | score:   0 | entropy: 0.271\n",
      "episode:  35 | score:  -3 | entropy: 0.276\n",
      "episode:  36 | score:   1 | entropy: 0.270\n",
      "episode:  37 | score:   0 | entropy: 0.271\n",
      "episode:  38 | score:   1 | entropy: 0.273\n",
      "episode:  39 | score:  -2 | entropy: 0.268\n",
      "episode:  40 | score:   0 | entropy: 0.271\n",
      "episode:  41 | score:   0 | entropy: 0.267\n",
      "episode:  42 | score:   1 | entropy: 0.268\n",
      "episode:  43 | score:  -1 | entropy: 0.268\n",
      "episode:  44 | score:   0 | entropy: 0.263\n",
      "episode:  45 | score:  -9 | entropy: 0.270\n",
      "episode:  46 | score:   0 | entropy: 0.267\n",
      "episode:  47 | score:  -1 | entropy: 0.272\n",
      "episode:  48 | score:   1 | entropy: 0.278\n",
      "episode:  49 | score:   1 | entropy: 0.273\n",
      "episode:  50 | score:  -7 | entropy: 0.272\n",
      "episode:  51 | score:   1 | entropy: 0.267\n",
      "episode:  52 | score:  -2 | entropy: 0.269\n",
      "episode:  53 | score:  -2 | entropy: 0.270\n",
      "episode:  54 | score:  -1 | entropy: 0.266\n",
      "episode:  55 | score:   1 | entropy: 0.269\n",
      "episode:  56 | score:   0 | entropy: 0.272\n",
      "episode:  57 | score:  -1 | entropy: 0.272\n",
      "episode:  58 | score:   0 | entropy: 0.270\n",
      "episode:  59 | score:   1 | entropy: 0.262\n",
      "episode:  60 | score:  -1 | entropy: 0.265\n",
      "episode:  61 | score:  -2 | entropy: 0.268\n",
      "episode:  62 | score:  -1 | entropy: 0.273\n",
      "episode:  63 | score:  -4 | entropy: 0.264\n",
      "episode:  64 | score:   0 | entropy: 0.260\n",
      "episode:  65 | score:  -8 | entropy: 0.269\n",
      "episode:  66 | score:  -1 | entropy: 0.267\n",
      "episode:  67 | score:   0 | entropy: 0.266\n",
      "episode:  68 | score:  -3 | entropy: 0.264\n",
      "episode:  69 | score:  -1 | entropy: 0.263\n",
      "episode:  70 | score:  -2 | entropy: 0.265\n",
      "episode:  71 | score:   0 | entropy: 0.267\n",
      "episode:  72 | score:   0 | entropy: 0.264\n",
      "episode:  73 | score:   1 | entropy: 0.264\n",
      "episode:  74 | score:   1 | entropy: 0.263\n",
      "episode:  75 | score:   0 | entropy: 0.261\n",
      "episode:  76 | score:   0 | entropy: 0.267\n",
      "episode:  77 | score:   0 | entropy: 0.260\n",
      "episode:  78 | score:   0 | entropy: 0.261\n",
      "episode:  79 | score:  -1 | entropy: 0.269\n",
      "episode:  80 | score:  -2 | entropy: 0.262\n",
      "episode:  81 | score:   0 | entropy: 0.258\n",
      "episode:  82 | score:   0 | entropy: 0.254\n",
      "episode:  83 | score:  -1 | entropy: 0.262\n",
      "episode:  84 | score:   1 | entropy: 0.259\n",
      "episode:  85 | score:   0 | entropy: 0.265\n",
      "episode:  86 | score:   1 | entropy: 0.258\n",
      "episode:  87 | score:   1 | entropy: 0.258\n",
      "episode:  88 | score:  -3 | entropy: 0.262\n",
      "episode:  89 | score:   0 | entropy: 0.263\n",
      "episode:  90 | score:   1 | entropy: 0.263\n",
      "episode:  91 | score:   1 | entropy: 0.257\n",
      "episode:  92 | score:  -1 | entropy: 0.270\n",
      "episode:  93 | score:   0 | entropy: 0.262\n",
      "episode:  94 | score:  -1 | entropy: 0.265\n",
      "episode:  95 | score:   0 | entropy: 0.255\n",
      "episode:  96 | score:   0 | entropy: 0.269\n",
      "episode:  97 | score:   0 | entropy: 0.265\n",
      "episode:  98 | score:   0 | entropy: 0.271\n",
      "episode:  99 | score:   0 | entropy: 0.255\n",
      "episode: 100 | score:   0 | entropy: 0.251\n",
      "episode: 101 | score:  -3 | entropy: 0.260\n",
      "episode: 102 | score:   0 | entropy: 0.263\n",
      "episode: 103 | score:   0 | entropy: 0.257\n",
      "episode: 104 | score:   1 | entropy: 0.258\n",
      "episode: 105 | score:   1 | entropy: 0.258\n",
      "episode: 106 | score:   0 | entropy: 0.252\n",
      "episode: 107 | score:   0 | entropy: 0.258\n",
      "episode: 108 | score:  -3 | entropy: 0.255\n",
      "episode: 109 | score:  -3 | entropy: 0.253\n",
      "episode: 110 | score:  -2 | entropy: 0.256\n",
      "episode: 111 | score:   0 | entropy: 0.249\n",
      "episode: 112 | score:   1 | entropy: 0.256\n",
      "episode: 113 | score:   0 | entropy: 0.256\n",
      "episode: 114 | score:  -1 | entropy: 0.270\n",
      "episode: 115 | score:   1 | entropy: 0.244\n",
      "episode: 116 | score:   0 | entropy: 0.251\n",
      "episode: 117 | score:   0 | entropy: 0.256\n",
      "episode: 118 | score:  -1 | entropy: 0.258\n",
      "episode: 119 | score:  -1 | entropy: 0.255\n",
      "episode: 120 | score:   1 | entropy: 0.279\n",
      "episode: 121 | score:   0 | entropy: 0.254\n",
      "episode: 122 | score:   0 | entropy: 0.245\n",
      "episode: 123 | score:  -1 | entropy: 0.267\n",
      "episode: 124 | score:   0 | entropy: 0.248\n",
      "episode: 125 | score:   1 | entropy: 0.242\n",
      "episode: 126 | score:   1 | entropy: 0.239\n",
      "episode: 127 | score:   0 | entropy: 0.261\n",
      "episode: 128 | score:   0 | entropy: 0.253\n",
      "episode: 129 | score:   1 | entropy: 0.234\n",
      "episode: 130 | score:  -1 | entropy: 0.263\n",
      "episode: 131 | score:   1 | entropy: 0.238\n",
      "episode: 132 | score:   1 | entropy: 0.227\n",
      "episode: 133 | score:   1 | entropy: 0.236\n",
      "episode: 134 | score:   0 | entropy: 0.250\n",
      "episode: 135 | score:   1 | entropy: 0.236\n",
      "episode: 136 | score:   1 | entropy: 0.228\n",
      "episode: 137 | score:   1 | entropy: 0.240\n",
      "episode: 138 | score:   1 | entropy: 0.228\n",
      "episode: 139 | score:   1 | entropy: 0.263\n",
      "episode: 140 | score:  -2 | entropy: 0.236\n",
      "episode: 141 | score:  -2 | entropy: 0.243\n",
      "episode: 142 | score:   0 | entropy: 0.242\n",
      "episode: 143 | score:   0 | entropy: 0.257\n",
      "episode: 144 | score:  -3 | entropy: 0.241\n",
      "episode: 145 | score:   1 | entropy: 0.229\n",
      "episode: 146 | score:  -1 | entropy: 0.241\n",
      "episode: 147 | score:   1 | entropy: 0.247\n",
      "episode: 148 | score:   1 | entropy: 0.225\n",
      "episode: 149 | score:   1 | entropy: 0.228\n",
      "episode: 150 | score:   0 | entropy: 0.225\n",
      "episode: 151 | score:   0 | entropy: 0.239\n",
      "episode: 152 | score:   0 | entropy: 0.229\n",
      "episode: 153 | score:  -1 | entropy: 0.220\n",
      "episode: 154 | score:   1 | entropy: 0.217\n",
      "episode: 155 | score:   0 | entropy: 0.252\n",
      "episode: 156 | score:   1 | entropy: 0.221\n",
      "episode: 157 | score:   1 | entropy: 0.212\n",
      "episode: 158 | score:   0 | entropy: 0.233\n",
      "episode: 159 | score:   1 | entropy: 0.219\n",
      "episode: 160 | score:  -2 | entropy: 0.259\n",
      "episode: 161 | score:   1 | entropy: 0.217\n",
      "episode: 162 | score:  -5 | entropy: 0.243\n",
      "episode: 163 | score:   1 | entropy: 0.212\n",
      "episode: 164 | score:  -2 | entropy: 0.244\n",
      "episode: 165 | score:   1 | entropy: 0.215\n",
      "episode: 166 | score:  -1 | entropy: 0.239\n",
      "episode: 167 | score:   1 | entropy: 0.198\n",
      "episode: 168 | score:   0 | entropy: 0.253\n",
      "episode: 169 | score:   0 | entropy: 0.240\n",
      "episode: 170 | score:   1 | entropy: 0.220\n",
      "episode: 171 | score:   1 | entropy: 0.216\n",
      "episode: 172 | score:   1 | entropy: 0.210\n",
      "episode: 173 | score:   1 | entropy: 0.219\n",
      "episode: 174 | score:   0 | entropy: 0.217\n",
      "episode: 175 | score:   0 | entropy: 0.225\n",
      "episode: 176 | score:   0 | entropy: 0.224\n",
      "episode: 177 | score:   1 | entropy: 0.211\n",
      "episode: 178 | score:   0 | entropy: 0.224\n",
      "episode: 179 | score:  -1 | entropy: 0.214\n",
      "episode: 180 | score:   0 | entropy: 0.235\n",
      "episode: 181 | score:   0 | entropy: 0.215\n",
      "episode: 182 | score:   0 | entropy: 0.239\n",
      "episode: 183 | score:   1 | entropy: 0.193\n",
      "episode: 184 | score:   0 | entropy: 0.210\n",
      "episode: 185 | score:  -1 | entropy: 0.220\n",
      "episode: 186 | score:   1 | entropy: 0.204\n",
      "episode: 187 | score:  -3 | entropy: 0.258\n",
      "episode: 188 | score:   0 | entropy: 0.226\n",
      "episode: 189 | score:   1 | entropy: 0.240\n",
      "episode: 190 | score:  -1 | entropy: 0.234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 191 | score:   0 | entropy: 0.241\n",
      "episode: 192 | score:   0 | entropy: 0.224\n",
      "episode: 193 | score:  -1 | entropy: 0.251\n",
      "episode: 194 | score:   1 | entropy: 0.200\n",
      "episode: 195 | score:  -1 | entropy: 0.240\n",
      "episode: 196 | score:   0 | entropy: 0.236\n",
      "episode: 197 | score:   0 | entropy: 0.251\n",
      "episode: 198 | score:   1 | entropy: 0.220\n",
      "episode: 199 | score:  -1 | entropy: 0.243\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "    # env 초기화\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    while not done:\n",
    "        # 현재 상태에 대한 행동 선택\n",
    "        action = agent.get_action(state)\n",
    "\n",
    "        # 선택한 행동으로 환경에서 한 타임스텝 진행 후 샘플 수집\n",
    "        next_state, reward, done = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "        agent.append_sample(state, action, reward)\n",
    "        score += reward\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            # 에피소드마다 정책신경망 업데이트\n",
    "            entropy = agent.train_model()\n",
    "            # 에피소드마다 학습 결과 출력\n",
    "            print(\"episode: {:3d} | score: {:3d} | entropy: {:.3f}\".format(\n",
    "                  e, score, entropy))\n",
    "\n",
    "            scores.append(score)\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, scores, 'b')\n",
    "            pylab.xlabel(\"episode\")\n",
    "            pylab.ylabel(\"score\")\n",
    "            pylab.savefig(\"./REINFORCEgraph.png\")\n",
    "\n",
    "\n",
    "    # 100 에피소드마다 모델 저장\n",
    "    if e % 100 == 0:\n",
    "        agent.model.save_weights('REINFORCEmodel', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.vstack([np.random.normal(4, 12, size=(5000,)), np.zeros(5000,)]).max(axis=0)\n",
    "x = np.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44.10202350206683, 6.997745250430584)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(x.mean()), np.mean(np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2.0",
   "language": "python",
   "name": "tensorflow2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
