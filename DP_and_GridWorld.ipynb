{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming and Grid-World\n",
    "- 강화 학습은 순차적으로 행동을 결정해야 하는 문제를 푸는 방법 중 하나\n",
    "- Process\n",
    "    - MDP 정의\n",
    "    - 벨만 방정식의 계산\n",
    "    - 최적 가치함수 + 최적 정책\n",
    "- 즉,\n",
    "    1. 순차적 행동 문제를 MDP로 전환\n",
    "    2. 가치함수를 벨만 방정식으로 반복적으로 계산\n",
    "    3. 최적 가치함수와 최적 정책을 찾는다.\n",
    "- 강화학습 등장 전, 순차적 의사결정 문제를 푸는 방법론, 다이나믹 프로그래밍 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 벨만 최적 방정식\n",
    "\n",
    "$$v_{\\ast}(s)=\\max_{a}E\\big[R_{t+1}+\\gamma v_{\\ast}(S_{t+1})\\;\\big|\\;S_t=s,A_t=a\\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 동적 프로그래밍\n",
    "- 리처드 벨만(Richard E. Bellman)이 1953년에 제시\n",
    "- 동적이라는 얘기는 그 말이 가리키는 대상이 시간에 따라 변한다는 것을 의미\n",
    "- 프로그래밍은 계획을 하는 것 자체를 의미.\n",
    "- 여러 프로세스가 다단계로 이뤄지는 것\n",
    "- 큰 문제를 바로 푸는 것이 아니라 작은 문제들을 풀어 나감"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming 종류\n",
    "- 정책 이터레이션 Policy Iteration\n",
    "- 가치 이터레이션 Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Policy Iteration\n",
    "\n",
    "- Random Policy -> Optimal Policy\n",
    "- 위를 위해 `평가`와 `발전`이 필요\n",
    "\n",
    "## Policy Evaluation\n",
    "- 정책 평가는 아래의 가치 함수로!\n",
    "$$v_\\pi(s)=E_\\pi\\big[\\sum_{k=t}^{\\infty}\\gamma^{k-t}R_{k+1}\\;\\big|\\;S_t=s\\big]$$\n",
    "- 물론 DP에서 환경에 대한 모든 정보를 알고 문제에 접근하기 때문에 위의 식을 계산할 수 있지만, 이는 비효율적이고 사실상 불가능\n",
    "- DP의 효과, 문제를 최대한 작게 쪼개고 작은 문제에 저장된 값들을 서로 이용해 계산하는 방식을 이용!\n",
    "- `벨만 기대 방정식`을 활용, 아래 식으로 가치 함수가 변형됨!\n",
    "$$v_\\pi(s)=E_\\pi\\big[R_{t+1}+\\gamma v_\\pi(S_{t+1})\\;\\big|\\;S_t=s\\big]$$\n",
    "- 위 식을 컴퓨터가 계산 가능하도록 기댓값, 확률적인 부분을 합의 형태로 변환\n",
    "$$v_\\pi(s)=\\sum_{a\\in A}\\pi(a|s)\\big(r_{(s,a)}+\\gamma v_\\pi(s^\\prime)\\big)$$\n",
    "\n",
    "## Policy Improvement\n",
    "- 굉장히 많은 방법들이 존재\n",
    "- 책에선 `Greedy Policy Improvement`를 소개\n",
    "- 초기의 정책은 무작위로 설정 (ex> [0.25, 0.25, 0.25, 0.25])\n",
    "- `q 함수`로 update\n",
    "$$q_\\pi(s,a)=E_\\pi\\big[R_{t+1}+\\gamma v_\\pi(S_{t+1})\\;\\big|\\;S_t=s,A_t=a\\big]$$\n",
    "- 위를 컴퓨터가 계산 가능하게 변형\n",
    "$$q_\\pi(s,a)=r_{(s,a)}+\\gamma v_\\pi(s^\\prime)$$\n",
    "- 위 값을 기반으로 정책을 발전시킴\n",
    "$$\\pi^{\\prime}(s)=\\text{argmax}_{a\\in A}q_\\pi(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Value Iteration\n",
    "- 정책 이터레이션은 명시적인 정책이 존재\n",
    "- 정책이 독립적이므로 결정적인 정책이 아니라 어떠한 정책도 가능 (확률적!)\n",
    "- 그러나, 최적 결정은 결정론적.\n",
    "- 현재의 가치함수가 최적은 아니지만 최적이라고 가정,\n",
    "- 가치함수에 대해 결정적인 형태의 정책을 적용한다면?\n",
    "- DP를 활용, 여러번 연산을 반복하며 최적 가치함수에 수렴, 최적 정책을 구할 거라는 기대!\n",
    "- 중요한 것,\n",
    "    - 정책이 명시적으로 표현되는 것이 아님\n",
    "    - 가치함수 안에 내재적(implicit)으로 포함돼있음\n",
    "\n",
    "## 벨만 최적 방정식과 가치 이터레이션\n",
    "- `벨만 기대 방정식`을 통해 전체 문제를 풀어서 나오는 답은? `현재 정책을 따라갔을 때 받을 참 보상`\n",
    "    1. 가치함수를 현재 정책에 대한 가치함수라 가정\n",
    "    2. 이를 반복적으로 계산\n",
    "    3. 현재 정책에 대한 참 가치함수가 됨\n",
    "$$v_\\pi(s)=E_\\pi\\big[R_{t+1}+\\gamma v_\\pi(S_{t+1})\\;\\big|\\;S_t=s\\big]$$\n",
    "\n",
    "- `벨만 최적 방정식`을 풀어 나오는 답은? `최적 가치함수`\n",
    "    1. 가치함수를 최적 정책에 대한 가치함수라 가정\n",
    "    2. 이를 반복적으로 계산\n",
    "    3. 결국 최적 가치 정책에 대한 참 가치함수, 즉 최적 가치함수를 찾음\n",
    "$$v_\\ast(s)=\\max_{a}E_\\pi\\big[R_{t+1}+\\gamma v_\\ast(S_{t+1})\\;\\big|\\;S_t=s,A_t=a\\big]$$\n",
    "\n",
    "- 때문에 value iteration에서는 정책 발전이 필요없음!\n",
    "    - 시작부터 최적의 정책이라고 가정했기 때문!\n",
    "\n",
    "- 벨만 최적 방정식에서 보면 `max`를 취함. 즉, 업데이트 시 세밀한 정책의 값을 고려할 필요가 X\n",
    "- 즉 현재 상태에서 가능한 $R_{t+1}+\\gamma v_k(S_{t+1})$의 값들 중 최고의 값으로 업데이트하면 됨\n",
    "\n",
    "$$v_{k+1}(s)=\\max_{a\\in A}\\big(r_{(s,a)}+\\gamma v_k(s^\\prime)\\big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드 출처: \n",
    "- https://github.com/rlcode/reinforcement-learning-kr-v2/tree/master/1-grid-world/1-policy-iteration\n",
    "- https://github.com/rlcode/reinforcement-learning-kr-v2/blob/master/1-grid-world/2-value-iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pi_environment import GraphicDisplay as PolicyGD\n",
    "from vi_environment import GraphicDisplay as ValueGD\n",
    "from vi_environment import Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        # 환경에 대한 객체 선언\n",
    "        self.env = env\n",
    "        # 가치함수를 2차원 리스트로 초기화\n",
    "        self.value_table = [[0.0] * env.width for _ in range(env.height)]\n",
    "        # 상 하 좌 우 동일한 확률로 정책 초기화\n",
    "        self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width\n",
    "                            for _ in range(env.height)]\n",
    "        # 마침 상태의 설정\n",
    "        self.policy_table[2][2] = []\n",
    "        # 할인율\n",
    "        self.discount_factor = 0.9\n",
    "\n",
    "    # 벨만 기대 방정식을 통해 다음 가치함수를 계산하는 정책 평가\n",
    "    def policy_evaluation(self):\n",
    "        # 다음 가치함수 초기화\n",
    "        next_value_table = [[0.00] * self.env.width\n",
    "                           for _ in range(self.env.height)]\n",
    "\n",
    "        # 모든 상태에 대해서 벨만 기대방정식을 계산\n",
    "        for state in self.env.get_all_states():\n",
    "            value = 0.0\n",
    "            # 마침 상태의 가치 함수 = 0\n",
    "            if state == [2, 2]:\n",
    "                next_value_table[state[0]][state[1]] = value\n",
    "                continue\n",
    "\n",
    "            # 벨만 기대 방정식\n",
    "            for action in self.env.possible_actions:\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                value += (self.get_policy(state)[action] *\n",
    "                          (reward + self.discount_factor * next_value))\n",
    "\n",
    "            next_value_table[state[0]][state[1]] = value\n",
    "\n",
    "        self.value_table = next_value_table\n",
    "\n",
    "    # 현재 가치 함수에 대해서 탐욕 정책 발전\n",
    "    def policy_improvement(self):\n",
    "        next_policy = self.policy_table\n",
    "        for state in self.env.get_all_states():\n",
    "            if state == [2, 2]:\n",
    "                continue\n",
    "            \n",
    "            value_list = []\n",
    "            # 반환할 정책 초기화\n",
    "            result = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "            # 모든 행동에 대해서 [보상 + (할인율 * 다음 상태 가치함수)] 계산\n",
    "            for index, action in enumerate(self.env.possible_actions):\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                value = reward + self.discount_factor * next_value\n",
    "                value_list.append(value)\n",
    "\n",
    "            # 받을 보상이 최대인 행동들에 대해 탐욕 정책 발전\n",
    "            max_idx_list = np.argwhere(value_list == np.amax(value_list))\n",
    "            max_idx_list = max_idx_list.flatten().tolist()\n",
    "            prob = 1 / len(max_idx_list)\n",
    "\n",
    "            for idx in max_idx_list:\n",
    "                result[idx] = prob\n",
    "\n",
    "            next_policy[state[0]][state[1]] = result\n",
    "\n",
    "        self.policy_table = next_policy\n",
    "\n",
    "    # 특정 상태에서 정책에 따라 무작위로 행동을 반환\n",
    "    def get_action(self, state):\n",
    "        policy = self.get_policy(state)\n",
    "        policy = np.array(policy)\n",
    "        return np.random.choice(4, 1, p=policy)[0]\n",
    "\n",
    "    # 상태에 따른 정책 반환\n",
    "    def get_policy(self, state):\n",
    "        return self.policy_table[state[0]][state[1]]\n",
    "\n",
    "    # 가치 함수의 값을 반환\n",
    "    def get_value(self, state):\n",
    "        return self.value_table[state[0]][state[1]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
