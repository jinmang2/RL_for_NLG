{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming and Grid-World\n",
    "- 강화 학습은 순차적으로 행동을 결정해야 하는 문제를 푸는 방법 중 하나\n",
    "- Process\n",
    "    - MDP 정의\n",
    "    - 벨만 방정식의 계산\n",
    "    - 최적 가치함수 + 최적 정책\n",
    "- 즉,\n",
    "    1. 순차적 행동 문제를 MDP로 전환\n",
    "    2. 가치함수를 벨만 방정식으로 반복적으로 계산\n",
    "    3. 최적 가치함수와 최적 정책을 찾는다.\n",
    "- 강화학습 등장 전, 순차적 의사결정 문제를 푸는 방법론, 다이나믹 프로그래밍 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 벨만 최적 방정식\n",
    "\n",
    "$$v_{\\ast}(s)=\\max_{a}E\\big[R_{t+1}+\\gamma v_{\\ast}(S_{t+1})\\;\\big|\\;S_t=s,A_t=a\\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 동적 프로그래밍\n",
    "- 리처드 벨만(Richard E. Bellman)이 1953년에 제시\n",
    "- 동적이라는 얘기는 그 말이 가리키는 대상이 시간에 따라 변한다는 것을 의미\n",
    "- 프로그래밍은 계획을 하는 것 자체를 의미.\n",
    "- 여러 프로세스가 다단계로 이뤄지는 것\n",
    "- 큰 문제를 바로 푸는 것이 아니라 작은 문제들을 풀어 나감"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming 종류\n",
    "- 정책 이터레이션 Policy Iteration\n",
    "- 가치 이터레이션 Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Policy Iteration\n",
    "\n",
    "- Random Policy -> Optimal Policy\n",
    "- 위를 위해 `평가`와 `발전`이 필요\n",
    "\n",
    "## Policy Evaluation\n",
    "- 정책 평가는 아래의 가치 함수로!\n",
    "$$v_\\pi(s)=E_\\pi\\big[\\sum_{k=t}^{\\infty}\\gamma^{k-t}R_{k+1}\\;\\big|\\;S_t=s\\big]$$\n",
    "- 물론 DP에서 환경에 대한 모든 정보를 알고 문제에 접근하기 때문에 위의 식을 계산할 수 있지만, 이는 비효율적이고 사실상 불가능\n",
    "- DP의 효과, 문제를 최대한 작게 쪼개고 작은 문제에 저장된 값들을 서로 이용해 계산하는 방식을 이용!\n",
    "- `벨만 기대 방정식`을 활용, 아래 식으로 가치 함수가 변형됨!\n",
    "$$v_\\pi(s)=E_\\pi\\big[R_{t+1}+\\gamma v_\\pi(S_{t+1})\\;\\big|\\;S_t=s\\big]$$\n",
    "- 위 식을 컴퓨터가 계산 가능하도록 기댓값, 확률적인 부분을 합의 형태로 변환\n",
    "$$v_\\pi(s)=\\sum_{a\\in A}\\pi(a|s)\\big(r_{(s,a)}+\\gamma v_\\pi(s^\\prime)\\big)$$\n",
    "\n",
    "## Policy Improvement\n",
    "- 굉장히 많은 방법들이 존재\n",
    "- 책에선 `Greedy Policy Improvement`를 소개\n",
    "- 초기의 정책은 무작위로 설정 (ex> [0.25, 0.25, 0.25, 0.25])\n",
    "- `q 함수`로 update\n",
    "$$q_\\pi(s,a)=E_\\pi\\big[R_{t+1}+\\gamma v_\\pi(S_{t+1})\\;\\big|\\;S_t=s,A_t=a\\big]$$\n",
    "- 위를 컴퓨터가 계산 가능하게 변형\n",
    "$$q_\\pi(s,a)=r_{(s,a)}+\\gamma v_\\pi(s^\\prime)$$\n",
    "- 위 값을 기반으로 정책을 발전시킴\n",
    "$$\\pi^{\\prime}(s)=\\text{argmax}_{a\\in A}q_\\pi(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Value Iteration\n",
    "- 정책 이터레이션은 명시적인 정책이 존재\n",
    "- 정책이 독립적이므로 결정적인 정책이 아니라 어떠한 정책도 가능 (확률적!)\n",
    "- 그러나, 최적 결정은 결정론적.\n",
    "- 현재의 가치함수가 최적은 아니지만 최적이라고 가정,\n",
    "- 가치함수에 대해 결정적인 형태의 정책을 적용한다면?\n",
    "- DP를 활용, 여러번 연산을 반복하며 최적 가치함수에 수렴, 최적 정책을 구할 거라는 기대!\n",
    "- 중요한 것,\n",
    "    - 정책이 명시적으로 표현되는 것이 아님\n",
    "    - 가치함수 안에 내재적(implicit)으로 포함돼있음\n",
    "\n",
    "## 벨만 최적 방정식과 가치 이터레이션\n",
    "- `벨만 기대 방정식`을 통해 전체 문제를 풀어서 나오는 답은? `현재 정책을 따라갔을 때 받을 참 보상`\n",
    "    1. 가치함수를 현재 정책에 대한 가치함수라 가정\n",
    "    2. 이를 반복적으로 계산\n",
    "    3. 현재 정책에 대한 참 가치함수가 됨\n",
    "$$v_\\pi(s)=E_\\pi\\big[R_{t+1}+\\gamma v_\\pi(S_{t+1})\\;\\big|\\;S_t=s\\big]$$\n",
    "\n",
    "- `벨만 최적 방정식`을 풀어 나오는 답은? `최적 가치함수`\n",
    "    1. 가치함수를 최적 정책에 대한 가치함수라 가정\n",
    "    2. 이를 반복적으로 계산\n",
    "    3. 결국 최적 가치 정책에 대한 참 가치함수, 즉 최적 가치함수를 찾음\n",
    "$$v_\\ast(s)=\\max_{a}E_\\pi\\big[R_{t+1}+\\gamma v_\\ast(S_{t+1})\\;\\big|\\;S_t=s,A_t=a\\big]$$\n",
    "\n",
    "- 때문에 value iteration에서는 정책 발전이 필요없음!\n",
    "    - 시작부터 최적의 정책이라고 가정했기 때문!\n",
    "\n",
    "- 벨만 최적 방정식에서 보면 `max`를 취함. 즉, 업데이트 시 세밀한 정책의 값을 고려할 필요가 X\n",
    "- 즉 현재 상태에서 가능한 $R_{t+1}+\\gamma v_k(S_{t+1})$의 값들 중 최고의 값으로 업데이트하면 됨\n",
    "\n",
    "$$v_{k+1}(s)=\\max_{a\\in A}\\big(r_{(s,a)}+\\gamma v_k(s^\\prime)\\big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드 출처: \n",
    "- https://github.com/rlcode/reinforcement-learning-kr-v2/tree/master/1-grid-world/1-policy-iteration\n",
    "- https://github.com/rlcode/reinforcement-learning-kr-v2/blob/master/1-grid-world/2-value-iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk # 내장 GUI module\n",
    "from tkinter import Button\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import ImageTk, Image\n",
    "import os\n",
    "\n",
    "\n",
    "PhotoImage = ImageTk.PhotoImage\n",
    "UNIT = 100  # 픽셀 수\n",
    "HEIGHT = 5  # 그리드월드 세로\n",
    "WIDTH = 5  # 그리드월드 가로\n",
    "TRANSITION_PROB = 1\n",
    "POSSIBLE_ACTIONS = [0, 1, 2, 3]  # 좌, 우, 상, 하\n",
    "ACTIONS = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # 좌표로 나타낸 행동\n",
    "REWARDS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    \n",
    "    \"\"\" 한정된 예제만을 위한, 작은 클래스 \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.transition_probability = TRANSITION_PROB\n",
    "        self.width = WIDTH  # Width of Grid World\n",
    "        self.height = HEIGHT  # Height of GridWorld\n",
    "        self.reward = [[0] * WIDTH for _ in range(HEIGHT)]\n",
    "        self.possible_actions = POSSIBLE_ACTIONS\n",
    "        self.reward[2][2] = 1  # (2,2) 좌표 동그라미 위치에 보상 1\n",
    "        self.reward[1][2] = -1  # (1,2) 좌표 세모 위치에 보상 -1\n",
    "        self.reward[2][1] = -1  # (2,1) 좌표 세모 위치에 보상 -1\n",
    "        self.all_state = []\n",
    "        # 상태 좌표 생성\n",
    "        for x in range(WIDTH):\n",
    "            for y in range(HEIGHT):\n",
    "                state = [x, y]\n",
    "                self.all_state.append(state)\n",
    "\n",
    "    def get_reward(self, state, action):\n",
    "        next_state = self.state_after_action(state, action)\n",
    "        return self.reward[next_state[0]][next_state[1]]\n",
    "\n",
    "    def state_after_action(self, state, action_index):\n",
    "        action = ACTIONS[action_index]\n",
    "        return self.check_boundary([state[0] + action[0], state[1] + action[1]])\n",
    "\n",
    "    @staticmethod\n",
    "    def check_boundary(state):\n",
    "        state[0] = (0 if state[0] < 0 else WIDTH - 1\n",
    "                    if state[0] > WIDTH - 1 else state[0])\n",
    "        state[1] = (0 if state[1] < 0 else HEIGHT - 1\n",
    "                    if state[1] > HEIGHT - 1 else state[1])\n",
    "        return state\n",
    "\n",
    "    def get_transition_prob(self, state, action):\n",
    "        return self.transition_probability\n",
    "\n",
    "    def get_all_states(self):\n",
    "        return self.all_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphicDisplay(tk.Tk):\n",
    "    \n",
    "    __img_path = \"../img/\"\n",
    "    \n",
    "    def __init__(self, agent, Env):\n",
    "        super(GraphicDisplay, self).__init__()\n",
    "        if isinstance(agent, ValueIteration):\n",
    "            self.title('Value Iteration')\n",
    "            self.evaluation_count = 0\n",
    "        elif isinstance(agent, PolicyIteration):\n",
    "            self.title('Policy Iteration')\n",
    "            self.iteration_count = 0\n",
    "        else:\n",
    "            raise ValueError('`agent` must be `ValueIteration` or `PolicyIteration`.')                \n",
    "        self.geometry('{0}x{1}'.format(HEIGHT * UNIT, HEIGHT * UNIT + 50))\n",
    "        self.texts = []\n",
    "        self.arrows = []\n",
    "        self.env = Env()\n",
    "        self.agent = agent\n",
    "        self.improvement_count = 0\n",
    "        self.is_moving = 0\n",
    "        (self.up, self.down, self.left, self.right), self.shapes = self.load_images()\n",
    "        self.canvas = self._build_canvas()\n",
    "        self.text_reward(2, 2, \"R : 1.0\")\n",
    "        self.text_reward(1, 2, \"R : -1.0\")\n",
    "        self.text_reward(2, 1, \"R : -1.0\")\n",
    "        \n",
    "    @property\n",
    "    def img_path(self):\n",
    "        return self.__img_path\n",
    "    \n",
    "    @img_path.setter\n",
    "    def img_path(self, path):\n",
    "        self.__img_path = path\n",
    "        \n",
    "    def _get_button_methods(self):\n",
    "        if isinstance(self.agent, PolicyIteration):\n",
    "            command1 = self.evaluate_policy\n",
    "            botton1 = 'Evaluate'\n",
    "            command2 = self.improve_policy\n",
    "            botton2 = 'Improve'\n",
    "            command3 = self.move_by_policy\n",
    "            botton3 = 'move'\n",
    "            command4 = self.reset\n",
    "            botton4 = 'reset'\n",
    "        else:\n",
    "            command1 = self.calculate_value\n",
    "            botton1 = 'Calculate'\n",
    "            command2 = self.print_optimal_policy\n",
    "            botton2 = 'Print Policy'\n",
    "            command3 = self.move_by_policy\n",
    "            botton3 = 'move'\n",
    "            command4 = self.clear\n",
    "            botton4 = 'clear'\n",
    "        return (\n",
    "            (command1, command2, command3, command4),\n",
    "            (button1,  button2,  button3,  button4),\n",
    "        )\n",
    "\n",
    "    def _build_canvas(self):\n",
    "        canvas = tk.Canvas(\n",
    "            self, bg='white', height=HEIGHT * UNIT, width=WIDTH * UNIT\n",
    "        )\n",
    "        # 버튼 초기화\n",
    "        iteration_button = Button(\n",
    "            self, text=\"Evaluate\", command=self.evaluate_policy)\n",
    "        iteration_button.configure(width=10, activebackground=\"#33B5E5\")\n",
    "        canvas.create_window(\n",
    "            WIDTH * UNIT * 0.13, HEIGHT * UNIT + 10, window=iteration_button)\n",
    "        policy_button = Button(self, text=\"Improve\",\n",
    "                               command=self.improve_policy)\n",
    "        policy_button.configure(width=10, activebackground=\"#33B5E5\")\n",
    "        canvas.create_window(WIDTH * UNIT * 0.37, HEIGHT * UNIT + 10,\n",
    "                             window=policy_button)\n",
    "        policy_button = Button(self, text=\"move\", command=self.move_by_policy)\n",
    "        policy_button.configure(width=10, activebackground=\"#33B5E5\")\n",
    "        canvas.create_window(WIDTH * UNIT * 0.62, HEIGHT * UNIT + 10,\n",
    "                             window=policy_button)\n",
    "        policy_button = Button(self, text=\"reset\", command=self.reset)\n",
    "        policy_button.configure(width=10, activebackground=\"#33B5E5\")\n",
    "        canvas.create_window(WIDTH * UNIT * 0.87, HEIGHT * UNIT + 10,\n",
    "                             window=policy_button)\n",
    "\n",
    "        # 그리드 생성\n",
    "        for col in range(0, WIDTH * UNIT, UNIT):  # 0~400 by 80\n",
    "            x0, y0, x1, y1 = col, 0, col, HEIGHT * UNIT\n",
    "            canvas.create_line(x0, y0, x1, y1)\n",
    "        for row in range(0, HEIGHT * UNIT, UNIT):  # 0~400 by 80\n",
    "            x0, y0, x1, y1 = 0, row, HEIGHT * UNIT, row\n",
    "            canvas.create_line(x0, y0, x1, y1)\n",
    "\n",
    "        # 캔버스에 이미지 추가\n",
    "        self.rectangle = canvas.create_image(50, 50, image=self.shapes[0])\n",
    "        canvas.create_image(250, 150, image=self.shapes[1])\n",
    "        canvas.create_image(150, 250, image=self.shapes[1])\n",
    "        canvas.create_image(250, 250, image=self.shapes[2])\n",
    "\n",
    "        canvas.pack()\n",
    "\n",
    "        return canvas\n",
    "    \n",
    "    def _load_image(self, filename, size):\n",
    "        return PhotoImage(\n",
    "            Image.open(os.path.join(self.img_path, filename))\n",
    "        ).resize(size)\n",
    "\n",
    "    def load_images(self):\n",
    "        up        = self._load_image('up.png', (13, 13))\n",
    "        right     = self._load_image('right.png', (13, 13))\n",
    "        left      = self._load_image('left.png', (13, 13))\n",
    "        down      = self._load_image('down.png', (13, 13))\n",
    "        rectangle = self._load_image('rectangle.png', (65, 65))\n",
    "        triangle  = self._load_image('triangle.png', (65, 65))\n",
    "        circle    = self._load_image('circle.png', (65, 65))\n",
    "        return (up, down, left, right), (rectangle, triangle, circle)\n",
    "    \n",
    "    def text_value(self, row, col, contents, font='Helvetica', size=10,\n",
    "                   style='normal', anchor=\"nw\"):\n",
    "        origin_x, origin_y = 85, 70\n",
    "        x, y = origin_y + (UNIT * col), origin_x + (UNIT * row)\n",
    "        font = (font, str(size), style)\n",
    "        text = self.canvas.create_text(x, y, fill=\"black\", text=contents,\n",
    "                                       font=font, anchor=anchor)\n",
    "        return self.texts.append(text)\n",
    "\n",
    "    def text_reward(self, row, col, contents, font='Helvetica', size=10,\n",
    "                    style='normal', anchor=\"nw\"):\n",
    "        origin_x, origin_y = 5, 5\n",
    "        x, y = origin_y + (UNIT * col), origin_x + (UNIT * row)\n",
    "        font = (font, str(size), style)\n",
    "        text = self.canvas.create_text(x, y, fill=\"black\", text=contents,\n",
    "                                       font=font, anchor=anchor)\n",
    "        return self.texts.append(text)\n",
    "\n",
    "    def rectangle_move(self, action):\n",
    "        base_action = np.array([0, 0])\n",
    "        location = self.find_rectangle()\n",
    "        self.render()\n",
    "        if action == 0 and location[0] > 0:  # 상\n",
    "            base_action[1] -= UNIT\n",
    "        elif action == 1 and location[0] < HEIGHT - 1:  # 하\n",
    "            base_action[1] += UNIT\n",
    "        elif action == 2 and location[1] > 0:  # 좌\n",
    "            base_action[0] -= UNIT\n",
    "        elif action == 3 and location[1] < WIDTH - 1:  # 우\n",
    "            base_action[0] += UNIT\n",
    "        # move agent\n",
    "        self.canvas.move(self.rectangle, base_action[0], base_action[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyEnv(GraphicDisplay):\n",
    "    \n",
    "    def __init__(self, agent, Env):\n",
    "        super(PolicyEnv, agent, Env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphicDisplay(tk.Tk):    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        if self.is_moving == 0:\n",
    "            self.evaluation_count = 0\n",
    "            self.improvement_count = 0\n",
    "            for i in self.texts:\n",
    "                self.canvas.delete(i)\n",
    "\n",
    "            for i in self.arrows:\n",
    "                self.canvas.delete(i)\n",
    "            self.agent.value_table = [[0.0] * WIDTH for _ in range(HEIGHT)]\n",
    "            self.agent.policy_table = ([[[0.25, 0.25, 0.25, 0.25]] * WIDTH\n",
    "                                        for _ in range(HEIGHT)])\n",
    "            self.agent.policy_table[2][2] = []\n",
    "            x, y = self.canvas.coords(self.rectangle)\n",
    "            self.canvas.move(self.rectangle, UNIT / 2 - x, UNIT / 2 - y)\n",
    "\n",
    "\n",
    "\n",
    "    def find_rectangle(self):\n",
    "        temp = self.canvas.coords(self.rectangle)\n",
    "        x = (temp[0] / 100) - 0.5\n",
    "        y = (temp[1] / 100) - 0.5\n",
    "        return int(y), int(x)\n",
    "\n",
    "    def move_by_policy(self):\n",
    "        if self.improvement_count != 0 and self.is_moving != 1:\n",
    "            self.is_moving = 1\n",
    "\n",
    "            x, y = self.canvas.coords(self.rectangle)\n",
    "            self.canvas.move(self.rectangle, UNIT / 2 - x, UNIT / 2 - y)\n",
    "\n",
    "            x, y = self.find_rectangle()\n",
    "            while len(self.agent.policy_table[x][y]) != 0:\n",
    "                self.after(100,\n",
    "                           self.rectangle_move(self.agent.get_action([x, y])))\n",
    "                x, y = self.find_rectangle()\n",
    "            self.is_moving = 0\n",
    "\n",
    "    def draw_one_arrow(self, col, row, policy):\n",
    "        if col == 2 and row == 2:\n",
    "            return\n",
    "\n",
    "        if policy[0] > 0:  # up\n",
    "            origin_x, origin_y = 50 + (UNIT * row), 10 + (UNIT * col)\n",
    "            self.arrows.append(self.canvas.create_image(origin_x, origin_y,\n",
    "                                                        image=self.up))\n",
    "        if policy[1] > 0:  # down\n",
    "            origin_x, origin_y = 50 + (UNIT * row), 90 + (UNIT * col)\n",
    "            self.arrows.append(self.canvas.create_image(origin_x, origin_y,\n",
    "                                                        image=self.down))\n",
    "        if policy[2] > 0:  # left\n",
    "            origin_x, origin_y = 10 + (UNIT * row), 50 + (UNIT * col)\n",
    "            self.arrows.append(self.canvas.create_image(origin_x, origin_y,\n",
    "                                                        image=self.left))\n",
    "        if policy[3] > 0:  # right\n",
    "            origin_x, origin_y = 90 + (UNIT * row), 50 + (UNIT * col)\n",
    "            self.arrows.append(self.canvas.create_image(origin_x, origin_y,\n",
    "                                                        image=self.right))\n",
    "\n",
    "    def draw_from_policy(self, policy_table):\n",
    "        for i in range(HEIGHT):\n",
    "            for j in range(WIDTH):\n",
    "                self.draw_one_arrow(i, j, policy_table[i][j])\n",
    "\n",
    "    def print_value_table(self, value_table):\n",
    "        for i in range(WIDTH):\n",
    "            for j in range(HEIGHT):\n",
    "                self.text_value(i, j, round(value_table[i][j], 2))\n",
    "\n",
    "    def render(self):\n",
    "        time.sleep(0.1)\n",
    "        self.canvas.tag_raise(self.rectangle)\n",
    "        self.update()\n",
    "\n",
    "    def evaluate_policy(self):\n",
    "        self.evaluation_count += 1\n",
    "        for i in self.texts:\n",
    "            self.canvas.delete(i)\n",
    "        self.agent.policy_evaluation()\n",
    "        self.print_value_table(self.agent.value_table)\n",
    "\n",
    "    def improve_policy(self):\n",
    "        self.improvement_count += 1\n",
    "        for i in self.arrows:\n",
    "            self.canvas.delete(i)\n",
    "        self.agent.policy_improvement()\n",
    "        self.draw_from_policy(self.agent.policy_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
